[toc]

# 神经网络

神经网络是深度学习的基础，正是深度学习的兴起，让停滞不前的人工智能再一次的取得飞速的发展。

其实神经网络的理论由来已久，灵感来自仿生智能计算，只是以前限于硬件的计算能力，没有突出的表现，直至谷歌的 AlphaGO 的出现，才让大家再次看到神经网络相较于传统机器学习的优异表现。

本文主要介绍神经网络中的重要基础概念，然后基于这些概念手工实现一个简单的神经网络。

## 1. 神经网络是什么

神经网络就像人脑一样，整体看上去非常复杂，但是其基础组成部分并不复杂。
其组成部分中最重要的就是 **神经元**（`neural`），**Sigmod 函数** 和 **层**（`layer`）。

### 1.1 神经元

神经元（`neural`）是神经网络最基本的元素，一个神经元包含 3 个部分：

- **获取输入**：获取多个输入的数据
- **数学处理**：对输入的数据进行数学计算
- **产生输出**：计算后**多个**输入数据变成**一个**输出数据

![1.1 神经元](./imgs/神经网络/1.1 神经元.png)

从上图中可以看出，**神经元**中的处理有 2 个步骤：

**第一个步骤：**从蓝色框变成红色框，是对输入的数据进行加权计算后合并为一个值（`N`）。$N = x_1w_1 + x_2w_2$ 其中，w~1~，w~2~ 分别是输入数据的权重。
一般在计算 *N* 的过程中，除了权重，还会加上一个偏移参数 *b*，最终得到：$N = x_1w_1 + x_2w_2 + b$

**第二个步骤**：从红色框变成绿色框，通过 **Sigmoid 函数**是对 `N` 进一步加工得到的神经元的最终输出（`M`）。

### 1.2 Sigmoid 函数

`Sigmoid` 函数也被称为 **S 函数**，因为的形状类似 **S 形**。

<img src="./imgs/神经网络/1.2 Sigmoid 函数.png" alt="1.2 Sigmoid 函数" style="zoom: 33%;" />

它是神经元中的重要函数，能够将输入数据的值映射到 $(0, 1)$ 之间。
最常用的 `Sigmoid` 函数是 $f(x) = \frac{1} {1 + e^{-x}}$，当然，不是只有这一种 `Sigmoid` 函数。

至此，神经元通过两个步骤，就把输入的多个数据，转换为一个 $(0, 1)$ 之间的值。

### 1.3 层

多个神经元可以组合成一层，一个神经网络一般包含一个输入层和一个输出层，以及多个隐藏层。

<img src="./imgs/神经网络/1.3 层.png" alt="1.3 层" style="zoom: 80%;" />

比如上图中，有 2 个隐藏层，每个隐藏层中分别有 4 个和 2 个神经元。
实际的神经网络中，**隐藏层数量**和其中的**神经元数量**都是不固定的，根据模型实际的效果来进行调整。

### 1.4 网络

通过神经元和层的组合就构成了一个网络，神经网络的名称由此而来。神经网络可大可小，可简单可复杂，不过，太过简单的神经网络模型效果一般不会太好。

因为一只果蝇就有 10 万个神经元，而人类的大脑则有大约 1000 亿个神经元，这就是为什么训练一个可用的神经网络模型需要庞大的算力，这也是为什么神经网络的理论 **1943 年** 就提出了，但是基于深度学习的 `AlphaGO` 却诞生于 **2015 年**。

## 2. 实现一个神经网络

了解上面的基本概念只能形成一个感性的认知。
下面通过自己动手实现一个最简单的神经网络，来进一步认识**神经元**，**Sigmoid 函数**以及**隐藏层**是如何发挥作用的。

### 2.1 准备数据

数据使用 `sklearn` 库中经典的鸢尾花数据集，这个数据集中有 3 个分类的鸢尾花，每个分类 50 条数据。

为了简化，只取其中前 `100` 条数据来使用，也就是取 **2 个分类**的鸢尾花数据。

```python
from sklearn.datasets import load_iris	# need pandas

ds = load_iris(as_frame=True, return_X_y=True)
data = ds[0].iloc[:100]
target = ds[1][:100]

print(data)
print(target)
```

变量 `data` 中 `100` 条数据，每条数据包含 **4 个属性**，分别是花萼的宽度和长度，花瓣的宽度和长度。

<img src="./imgs/神经网络/鸢尾花数据1.png" alt="鸢尾花数据1" style="zoom: 67%;" />

变量 `target` 中也是 `100` 条数据，只有 **0 和 1** 两种值，表示两种不同种类的鸢尾花。

<img src="./imgs/神经网络/鸢尾花数据2.png" alt="鸢尾花数据2" style="zoom: 67%;" />

### 2.2 实现神经元

准备好了数据，下面开始逐步实现一个简单的神经网络。首先，实现最基本的单元——**神经元**。

本文第一节中已经介绍了神经元中主要的2个步骤，分别计算出 $N$ 和 $M$​。

计算 $N$ 时，依据每个输入元素的权重 $(w_1, w_2)$ 和整体的偏移 $b$；
计算 $M$ 时，通过 `Sigmoid` 函数。

![1.1 神经元](./imgs/神经网络/1.1 神经元.png)

```python
from dataclasses import dataclass, field

import numpy as np


def sigmoid(x):
    return 1 / (1 + np.exp(-1 * x))


@dataclass
class Neuron:   # 神经元
    weights: list[float] = field(default_factory=lambda: [])    # 权重
    bias: float = 0.0   # 偏移
    N: float = 0.0      # 保存 wx + b
    M: float = 0.0      # 保存 sigmod(wx + b)

    def compute(self, inputs):
        self.N = np.dot(self.weights, inputs) + self.bias
        self.M = sigmoid(self.N)
        return self.M
```

上面的代码中，`Neuron` 类表示神经元，这个类有4个属性：

其中属性 `weights` 和 `bias` 是计算时的权重和偏移；
属性 `N` 和 `M` 分别是神经元中两步计算的结果。

`Neuron` 类的 `compute()` 方法根据输入的数据计算神经元的输出。

### 2.3 实现神经网络

神经元实现之后，下面就是构建神经网络。

我们的输入数据是带有 **4 个属性**（花萼的宽度和长度，花瓣的宽度和长度）的鸢尾花数据，所以神经网络的输入层有 4 个值 $(x_1, x_2, x_3, x_4)$。

为了简单起见，我们的神经网络只构建一个**隐藏层**，其中包含 **3 个神经元**。
最后就是输出层，输出层最后输出一个值，表示鸢尾花的种类。

由此形成的简单神经网络如下图所示：

![2.3 实现神经网络](./imgs/神经网络/2.3 实现神经网络.png)

```python
# 自定义神经网络
@dataclass()
class MyNeuronNetwork:
    # field
    # 隐藏层 3 个神经元
    HL1: Neuron = field(init=False)
    HL2: Neuron = field(init=False)
    HL3: Neuron = field(init=False)
    # 输出层 1 个神经元
    O1: Neuron = field(init=False)

    """
    __post_init__是Python 3.7新增的特殊方法，用于取代__init__方法中常见的属性初始化代码。

    它的作用是在类被实例化完成后，对属性进行进一步的初始化或操作，并且只会在实例化时被调用一次。
    这样可以减少__init__中的代码量，使其更加简洁。
    """
    def __post_init__(self):
        """
        np.random.dirichlet(): 从狄利克雷分布中抽取样本。
            alpha：它是一个浮点数或一维数组。
                当 alpha 是一个浮点数时，生成的随机数将服从所有维度上具有相同形状的 Dirichlet 分布
                当 alpha 是一个一维数组时，生成的随机数将服从具有不同形状的 Dirichlet 分布，各维度的形状由 alpha 数组指定。
            size：输出的形状。
        np.random.normal(): 返回一组符合高斯分布的概率密度随机数。
            loc(float)：此概率分布的均值（对应着整个分布的中心 center）
            scale(float)：此概率分布的标准差（对应于分布的宽度，scale 越大，图形越矮胖；scale 越小，图形越瘦高）
            size(int or tuple of ints)：输出的 shape，默认为 None，只输出一个值
        """
        self.HL1 = Neuron()
        self.HL1.weights = np.random.dirichlet(np.ones(4))  # np.ones(4): 返回一个全 1 的数组
        self.HL1.bias = np.random.normal()

        self.HL2 = Neuron()
        self.HL2.weights = np.random.dirichlet(np.ones(4))
        self.HL2.bias = np.random.normal()

        self.HL3 = Neuron()
        self.HL3.weights = np.random.dirichlet(np.ones(4))
        self.HL3.bias = np.random.normal()

        self.O1 = Neuron()
        self.O1.weights = np.random.dirichlet(np.ones(3))
        self.O1.bias = np.random.normal()

    def compute(self, inputs):
        m1 = self.HL1.compute(inputs)
        m2 = self.HL2.compute(inputs)
        m3 = self.HL3.compute(inputs)

        output = self.O1.compute([m1, m2, m3])
        return output
```

`MyNeuronNetwork` 类是自定义的神经网络，其中的属性是 **4 个神经元**。
`HL1`，`HL2`，`HL3` 是**隐藏层**的 3 个神经元；`O1` 是**输出层**的神经元。

`__post__init__` 函数是为了初始化各个神经元。
因为输入层是 4 个属性 $(x_1, x_2, x_3, x_4)$，所以神经元 `HL1`，`HL2`，`HL3` 的 `weights` 初始化为 **4 个**随机数组成的列表，偏移（`bias`）用 1 个随机数来初始化。

对于神经元 `O1`，它的输入是隐藏层的 3 个神经元，所以它的 `weights` 初始化为 **3 个**随机数组成的列表，偏移（`bias`）还是用 1 个随机数来初始化。

最后还有一个 `compute` 函数，这个函数描述的就是整个神经网络的计算过程。
首先，根据输入层 $(x_1, x_2, x_3, x_4)$ 的数据计算隐藏层的神经元（`HL1`，`HL2`，`HL3`）；
然后，以隐藏层的神经元（`HL1`，`HL2`，`HL3`）的输出作为输出层的神经元（`O1`）的输入，并将`O1`的计算结果作为整个神经网络的输出。

### 2.4 训练模型

上面的神经网络中各个神经元的中的参数（主要是 `weights` 和 `bias`）都是随机生成的，所以直接使用这个神经网络，效果一定不会很好。

所以，我们需要给神经网络（`MyNeuronNetwork`类）加一个训练函数，用来训练神经网络中各个神经元的参数（也就是个各个神经元中的 `weights` 和 `bias`）。

```python
@dataclass
class MyNeuronNetwork:
    # ...
    
    # 训练: 使用 随机梯度下降算法
    def train(self, data: pd.DataFrame, target: pd.Series):
        learn_rate = 0.1    # 学习率
        epochs = 100    # 轮次

        for epoch in range(epochs):
            # 对数据集中的每条样本进行迭代：
            for x, y_true in zip(data.values, target):
                # 前向传播。计算每一个的神经元的输出
                self.HL1.compute(x)
                self.HL2.compute(x)
                self.HL3.compute(x)
                self.O1.compute([self.HL1.M, self.HL2.M, self.HL3.M])

                y_pred = self.O1.M  # 预测值

                # loss(w, b) = (y - y_hat) ** 2
                d_ypred = -2 * (y_true - y_pred)    # 计算输出层的误差，使用均方误差损失函数的负梯度。

                # 反向传播。计算各层误差对权重和偏置的影响
                # 输出层 O1 的权重和偏置的误差导数。这里的 3 个输入分别来自 3 个隐藏层的输出。
                d_ypred_o1_weights = [
                    self.HL1.M * deriv_sigmoid(self.O1.N),
                    self.HL2.M * deriv_sigmoid(self.O1.N),
                    self.HL3.M * deriv_sigmoid(self.O1.N),
                ]
                d_ypred_o1_b = deriv_sigmoid(self.O1.N)

                # 计算隐藏层神经元 HL1, HL2, HL3 的误差导数，用于更新权重和偏置。
                d_ypred_hl1 = self.O1.weights[0] * deriv_sigmoid(self.O1.N)
                d_ypred_hl2 = self.O1.weights[1] * deriv_sigmoid(self.O1.N)
                d_ypred_hl3 = self.O1.weights[2] * deriv_sigmoid(self.O1.N)

                # 计算神经元 HL1 的权误差导数。
                d_hl1_weights = [
                    x[0] * deriv_sigmoid(self.HL1.N),
                    x[1] * deriv_sigmoid(self.HL1.N),
                    x[2] * deriv_sigmoid(self.HL1.N),
                    x[3] * deriv_sigmoid(self.HL1.N),
                ]
                d_hl1_b = deriv_sigmoid(self.HL1.N)

                # 计算神经元 HL2 的权误差导数。
                d_hl2_weights = [
                    x[0] * deriv_sigmoid(self.HL2.N),
                    x[1] * deriv_sigmoid(self.HL2.N),
                    x[2] * deriv_sigmoid(self.HL2.N),
                    x[3] * deriv_sigmoid(self.HL2.N),
                ]
                d_hl2_b = deriv_sigmoid(self.HL2.N)

                # 计算神经元 HL3 的权误差导数。
                d_hl3_weights = [
                    x[0] * deriv_sigmoid(self.HL3.N),
                    x[1] * deriv_sigmoid(self.HL3.N),
                    x[2] * deriv_sigmoid(self.HL3.N),
                    x[3] * deriv_sigmoid(self.HL3.N),
                ]
                d_hl3_b = deriv_sigmoid(self.HL3.N)

                # 使用误差导数和学习率来更新神经元 HL1 的权重和偏置。
                # Neuron hl1
                w = learn_rate * d_ypred * d_ypred_hl1
                for i in range(len(self.HL1.weights)):
                    self.HL1.weights[i] -= w * d_hl1_weights[i]

                self.HL1.bias -= w * d_hl1_b

                # Neuron hl2
                w = learn_rate * d_ypred * d_ypred_hl2
                for i in range(len(self.HL2.weights)):
                    self.HL2.weights[i] -= w * d_hl2_weights[i]

                self.HL2.bias -= w * d_hl2_b

                # Neuron hl3
                w = learn_rate * d_ypred * d_ypred_hl3
                for i in range(len(self.HL3.weights)):
                    self.HL3.weights[i] -= w * d_hl3_weights[i]

                self.HL3.bias -= w * d_hl3_b

                # Neuron o1
                w = learn_rate * d_ypred
                for i in range(len(self.O1.weights)):
                    self.O1.weights[i] -= w * d_ypred_o1_weights[i]

                self.O1.bias -= w * d_ypred_o1_b
```

上面的 `train` 函数有两个参数 `data`（训练数据）和 `target`（训练数据的标签）。
我们使用**随机梯度下降算法**来训练模型的参数。

此外，再实现一个预测函数 `predict`，传入测试数据集，然后用我们训练好的神经网络模型来预测测试数据集的标签。

```python
    # 预测
    def predict(self, data: pd.DataFrame):
        results = []
        for idx, row in enumerate(data.values):
            pred = self.compute(row)
            results.append(round(pred))

        return results
```

### 2.5 验证模型效果

```python
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

from neuron_network import MyNeuronNetwork, Neuron


def main():
    # 加载数据
    ds = load_iris(as_frame=True, return_X_y=True)

    # 只用前 100 条数据
    data = ds[0].iloc[:100]
    target = ds[1][:100]

    # 划分训练数据、测试数据
    # test_size = 0.2: 表示 80% 用作训练数据，剩余 20% 用作测试数据
    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)

    # 创建神经网络
    nn = MyNeuronNetwork()

    # 训练数据集
    nn.train(X_train, y_train)

    # 检验模型
    result = nn.predict(X_test)
    df = pd.DataFrame()
    df["预测值"] = result
    df["实际值"] = y_test.values
    print(df)


if __name__ == "__main__":
    main()

```



## 3. 总结

本文中的的神经网络示例是为了介绍神经网络的一些基本概念，所以对神经网络做了尽可能的简化，为了方便去手工实现。

而实际环境中的神经网络，不仅神经元的个数，隐藏层的数量极其庞大，而且其计算和训练的方式也很复杂，手工去实现不太可能，一般会借助 `TensorFlow`，`Keras` 和 `PyTorch` 等等知名的 Python 深度学习库来帮助我们实现。